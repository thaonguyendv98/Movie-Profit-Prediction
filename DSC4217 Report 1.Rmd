---
title: "DSC4217 Report"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: hide
    number_sections: true
    theme: cosmo
    highlight: tango  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#libraries
library(knitr)

```

#Background
The Godfather Capital Group is an American investment company that has been actively involved in the stock market since its establishment in 1990. Survived through two devastating financial crisis in 1997 and 2008 through applying diversification strategy in stock selection, the company is proud of its risk management techniques.
 
However, In 2018,the Dow fell 5.6%. The S&P 500 was down 6.2% and the Nasdaq fell 4%. It was the worst year for stocks since 2008. The CEO of the firm, Jiang Tao is worried about a possible upcoming crisis in the stock market and thus wants to further diversify the company's investment portfolio by entering into the movie industry.

<center><img src="https://i.imgur.com/hmWjriN.png" style="width: 600px;"/></center>

As such, a new functional team named 'DSC4217' team has benn formed to carry out research and data analysis for this new line of investment. After some initial analysis, the group noticed the following findings: 

1. The average movie production cost is around $65 million, while the follow-up marketing and distribution is around $35 million. In total, the cost to product a movie is at $100 million.
2. Film has a reputation of being highly risky and only a very small number become blockbusters. 

In conclusion, the large volume of investment required and the high risk embedded makes investment in movie industry highly risky. Thus, to better assess the risk associated with each production candidate, the team aims to build a classification model as well as a regression model to to forecast the likelihood of profiting from the investment and the volume of the profits respectively. 

<b>Some additional information:</b>

1. As it is an American company and majority of its employees only understand English, the company only wants to invest in English-Language movie. 
2. The company CFO,Run, is a die hard fan for Bruce Lee and has strong passion in action movies. Unless model shows otherwise, Run will only permits fund request for investment in action movies. 

#Data Source
The data used in this report were obtained from two sources for the time period 1980 to 2016, 5000 movie entries from IMDB while the average ticket price is sourced from the Box Office Mojo website.

##Data Variable
The IMDB data came with 5000 entries with 29 variables (14 numerical variables and 15 categorical variables). The yearly prices and number of movies realeased yearly were added from Box Office Mojo. 

<b>Below is the list of variables name and their description:</b>
```{r}
#variables <- read.csv("./data variables.csv")
#kable(variables,caption="List of variables")
```

##Data Preperation
```{r}
```


#Classification

In order to help our investors in choosing the right movie to invest, we decide to develop classification model to predict profitability given other factors. Our dependent variable is profitability, which is a binary column to indicate Profitable (1) or Not Profitable (2) given that Profit = gross - budget.
In this case, our main evaluation criteria is accuracy when validating agaisnt training dataset, testing dataset and k-fold cross validation.

```{r}
#load library
library(caret)
library(ggplot2)
library(rpart)
library(randomForest)
library(rpart.plot)
library(grid)
library(mvtnorm)
library(modeltools)
library(stats4)
library(strucchange)
library(zoo)
library(party)
```

```{r}
#load cleaned data
dataset <-read.csv('C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/imdb_cleaned.csv')
dataset$profitability <- as.factor(dataset$profitability)
```

Predictor variables we have choosen for the classification model are
X1: num_critic_for_reviews
X2: duration
X3: director_facebook_likes <when is N.A, just use average to replace missing data>
X4: num_voted_users
X5: cast_total_facebook_likes (for 3 casts, and each missing individual cast is filled with average value)
X6: num_user_for_reviews
X7~X29(Use action as baseline): Action, Adventure, Animation, Biography, Comedy, Crime, Documentary, Drama,	Family, Fantasy, History, Horror, Music, Musical, Mystery, Romance, Sci.Fi,Sport,Thriller,War,Western
X30: # ofMovies
X31: Avg.Ticket Price
X32: Content_rating 
X33: Budget
X34: imdb_score

```{r}
#Subseting dataset with chosen dependent and predictor variables
dataset1<-dataset[,c(53,5,6,7,14,15,19,27:51,22,23,25)]
head(dataset1)
```


```{r}
table(dataset1$profitability)
```

In order to validate our model, we split the dataset into two sets, training dataset (80%) and testing dataset (20%). 
```{r}
set.seed(1234)
sample <- sample(c(TRUE,FALSE),nrow(dataset1),prob = c(0.8,0.2),replace = TRUE)
movie.train <- dataset1[sample, ]
movie.test <- dataset1[!sample, ]
```

###K-forld classification
To ensure accuracy, we use 5-fold cross validation to make sure that our model is relatively stable and not overfitting. Therefore, the accuracy when testing against 5 folds will be taken into consideration when choosing the final model.
```{r}
set.seed(123)
train_control <- trainControl(method="cv", number=5,savePredictions = TRUE)#,summaryFunction=defaultSummary(trainData,trainData$DELAY))
```

##Model 1: Classical decision tree
```{r}
#classification tree
#classtree <- train(profitability~., data=movie.train, trControl=train_control, method="rpart")
#saveRDS(classtree, file = "C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/classtree.rds")
classtree<-readRDS("C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/classtree.rds")
library(rpart.plot)
prp(classtree$finalModel)            #plot the final tree model
```
The decision tree can be intepreted that the movie is more likely to be profitable if the number of voted users is larger than 56000, which is intuitively correct as the movie which has larger number of voted users is usually more popular. Next, the movie is more likely to be profitable if the average ticket price is larger than 4.2. Noted that in this case average ticket price is an indicator of inflation. Therefore, higher average ticket price means higher inflation, meaning recently launched movies are more profitable. 
```{r}
print(classtree)                                    #print tree performance
```

```{r}
ggplot(classtree)                                   #plot tree performance
```
Indeed, cp of 0.01635514 is chosen as increasing complexity parameter leads to lower accuracy.
```{r}
classtree$resample                                  #reporting each fold performance
```

The performance is relatively stable with accuracy against five fold within 69%-70% and Kappy within 38%-42%.
```{r}
#Confusion matrix with test data
cm1<-as.matrix(table(predict(classtree,movie.test),movie.test$profitability))
cm1
```

```{r}
#Calculating
accuracy11<-(cm1[1,1]+cm1[2,2])/(cm1[1,1]+cm1[2,2]+cm1[1,2]+cm1[2,1])
accuracy11
```

```{r}
#Confusion matrix with train data
cm2<-as.matrix(table(predict(classtree,movie.train),movie.train$profitability))
cm2
```


```{r}
accuracy12<-(cm2[1,1]+cm2[2,2])/(cm2[1,1]+cm2[2,2]+cm2[1,2]+cm2[2,1])
accuracy12
```

From the confusion matrix, we can conclude that the model also can predict accurately around 74% of both testing data and training data

```{r}
trellis.par.set(caretTheme())
densityplot(classtree, pch = "|")                   #density plot
```

Generally, the accuracy of the model falls in the range of 66%-74% with the distribution above.



##Model 2: Conditional Inference Classification tree
```{r}
#classification tree
#classtree1 <- train(profitability~., data=movie.train, trControl=train_control, method="ctree")
#saveRDS(classtree1, file = "C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/classtree1.rds")
classtree1<-readRDS("C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/classtree1.rds")

```

```{r}
plot(classtree1$finalModel)            #plot the final tree model
```
For instance, if the number of number of voted users <= 56000, the movie is more likely to be popular and thus not profitable. For movies with number voted users <= 3875, the movie is even more likely to be not profitable with budget less than 50000 USD. 
```{r}
print(classtree1$finalModel)
```
For example, given the number of voted users > 244566, the movie is more likely to be profitable with Comedy genre combined with Fantasy genre.


```{r}
print(classtree1)                                    #print tree performance
```

```{r}
ggplot(classtree1)                                   #plot tree performance
```
With higher P-value Threshold, the performance of the model is decreased, demonstrated by lower accuracy.
```{r}
classtree1$resample                                  #reporting each fold performance
```
Conditional Inference Tree provides accuracy in the range of 69% to 71% and Kappy in the range of 38% to 43% when testing against 5 folds, which will be used in making the final decision.
```{r}
#Confusion matrix with test data
cm1<-as.matrix(table(predict(classtree1,movie.test),movie.test$profitability))
cm1
```

```{r}
#Calculating accuracy rate
accuracy21<-(cm1[1,1]+cm1[2,2])/(cm1[1,1]+cm1[2,2]+cm1[1,2]+cm1[2,1])
accuracy21
```

```{r}
#Confusion matrix with train data
cm2<-as.matrix(table(predict(classtree1,movie.train),movie.train$profitability))
cm2
```


```{r}
#Calculating accuracy rate
accuracy22<-(cm2[1,1]+cm2[2,2])/(cm2[1,1]+cm2[2,2]+cm2[1,2]+cm2[2,1])
accuracy22
```
From the confusion matrices, we can conclude that the model perform quite well with the training dataset (82% accuracy) and not as well with the testing dataset (71% accuracy).

```{r}
trellis.par.set(caretTheme())
densityplot(classtree1, pch = "|")                   #density plot
```
Generally the accuracy usually falls at around 69% with the distribution shown as above.

##Model 3: Log regression model
```{r}
#classlogit <- train(profitability~., data=movie.train, trControl=train_control, method="glmStepAIC")
#saveRDS(classlogit, file = "C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/classlogit.rds")
classlogit<-readRDS("C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/classlogit.rds")
```

```{r}
print(classlogit$finalModel)                        #print the final logit model
```
It is recognizable that genres such as Comedy, Documentary, Family, Horror, Romanc, Short and Music are positively correlated with profitability. Also, content rating PG is recommended as it is also positively correlated with profitability. IMDB score is also positively correlated while budget have very small impact on profitability.
```{r}
classlogit$resample                                 #reporting each fold performance
```
Generally, when testing against 5 folds, the accuracy rate falls within 67% to 74% and Kappy falls within 35% to 47%. 
```{r}
#Confusion matrix with test data
cm1<-as.matrix(table(predict(classlogit,movie.test),movie.test$profitability))
cm1
```

```{r}
accuracy31<-(cm1[1,1]+cm1[2,2])/(cm1[1,1]+cm1[2,2]+cm1[1,2]+cm1[2,1])
accuracy31
```

```{r}
#Confusion matrix with train data
cm2<-as.matrix(table(predict(classlogit,movie.train),movie.train$profitability))
cm2
```


```{r}
accuracy32<-(cm2[1,1]+cm2[2,2])/(cm2[1,1]+cm2[2,2]+cm2[1,2]+cm2[2,1])
accuracy32
```

```{r}
densityplot(classlogit, pch = "|")                   #density plot
```
Overall in conclusion, the model performs stably when predicting training data and testing data with around 72% accuracy, derived from the confusion matrices above and overall, the accuracy rate tends to be 72% with distribution shown above.

##Model 4: Random Forest
```{r}
train_control <- trainControl(method="cv", number=5,savePredictions = TRUE)#,summaryFunction=defaultSummary(trainData,trainData$DELAY))

library(caret)
#rfmodel<-train(profitability~., data=movie.train, trControl=train_control, method="rf",tuneLength=15)
varImp(rfmodel)
```

```{r}
#saveRDS(rfmodel, file = "C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/rfmodel1.rds")
rfmodel<-readRDS("C:/Users/Linh/Documents/Y3 Sem 2/DSC4217/Group Project/New data cleaned/rfmodel1.rds")
```

```{r}
print(rfmodel$finalModel)                     #print the final random forest model
```

```{r}
rfmodel$resample                                 #reporting each fold performance
```
Generally, when testing against 5 folds, the accuracy rate falls between 73% to 78% and Kappa falls between 46% and 56%.
```{r}
#Confusion matrix with test data
cm1<-as.matrix(table(predict(rfmodel,movie.test),movie.test$profitability))
cm1
```

```{r}
accuracy41<-(cm1[1,1]+cm1[2,2])/(cm1[1,1]+cm1[2,2]+cm1[1,2]+cm1[2,1])
accuracy41
```

```{r}
#Confusion matrix with train data
cm2<-as.matrix(table(predict(rfmodel,movie.train),movie.train$profitability))
cm2
```

```{r}
accuracy42<-(cm2[1,1]+cm2[2,2])/(cm2[1,1]+cm2[2,2]+cm2[1,2]+cm2[2,1])
accuracy42
```

```{r}
densityplot(rfmodel, pch = "|")                   #density plot
```
In conclusion, the model can predict the training dataset 100% accurately and predict the testing dataset 77% accurately. Overall, the model accuracy usually falls between 70% and 82% with distribution as above.


##Summary and conclusion
```{r}
# collect resamples
results <- resamples(list(dtree=classtree, ctree=classtree1 ,glm=classlogit,rf=rfmodel))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
dotplot(results)
```

```{r}
results.df<- data.frame(accuracy =c("testing","training"), classification_decision_tree = c(accuracy11,accuracy12),conditional_inference_tree = c(accuracy21,accuracy22), log_regression =  c(accuracy31,accuracy32), random_forest= c(accuracy41,accuracy42))
print(results.df)
```
Random forest model is chosen as the final model used to predict profitability as it has the highest accuracy rate when predicting both train and test dataset. Also, the model solve the problem of overfitting, given that it is stable against 5 fold cross validation without sacrificing the accuracy rate.


